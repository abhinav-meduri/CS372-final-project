# Code Walkthrough Part 5: Validation, Evaluation, and Inference Pipeline

## Overview

This final part covers:
1. Validation loop during training
2. Evaluation metrics (7 different metrics)
3. Complete inference pipeline (`patent_analyzer.py`)
4. Hybrid RAG architecture
5. End-to-end workflow example

---

# PART 1: VALIDATION LOOP

## Continuing from Training Loop

After each training epoch, we validate:

```python
if val_loader is not None:
    # Switch to evaluation mode
    self.model.eval()
    
    val_loss = 0.0
    all_preds = []
    all_labels = []
    
    # Disable gradient computation (saves memory and speeds up)
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X = batch_X.to(self.device)
            batch_y = batch_y.to(self.device)
            
            # Forward pass only (no backward)
            outputs = self.model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # Accumulate
            val_loss += loss.item()
            all_preds.extend(outputs.cpu().numpy())
            all_labels.extend(batch_y.cpu().numpy())
```

### Model Evaluation Mode

```python
self.model.eval()
```

**What changes in eval mode:**

1. **Dropout is disabled:**
```python
# Training mode (train=True):
x = Dropout(x)  # Randomly zeros 30% of values

# Eval mode (train=False):
x = Dropout(x)  # Does nothing (identity)
```

2. **BatchNorm uses running statistics:**
```python
# Training mode:
mean = batch_mean  # Statistics from current batch
std = batch_std

# Eval mode:
mean = running_mean  # Running average from training
std = running_std
```

**Why these differences?**

**Dropout:**
- Training: Regularizes by adding noise
- Inference: Want deterministic predictions (no randomness)

**BatchNorm:**
- Training: Batch might have weird statistics by chance
- Inference: Might have batch_size=1 (can't compute mean/std)

### No Gradient Computation

```python
with torch.no_grad():
    outputs = self.model(batch_X)
```

**What `torch.no_grad()` does:**

Disables autograd (gradient tracking):

```python
# With autograd (training):
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x ** 2
y.backward()  # Computes gradients
print(x.grad)  # tensor([2.0, 4.0])

# With no_grad (inference):
with torch.no_grad():
    x = torch.tensor([1.0, 2.0])
    y = x ** 2
    # y has no computational graph
    # Can't call y.backward()
```

**Benefits:**

1. **Memory savings:**
   - No computation graph stored
   - For deep networks: ~2-3Ã— less memory

2. **Speed:**
   - No gradient bookkeeping
   - Faster forward pass (~1.5Ã— speedup)

**Validation is 1.5-2Ã— faster than training per batch.**

### Collecting Predictions

```python
all_preds.extend(outputs.cpu().numpy())
all_labels.extend(batch_y.cpu().numpy())
```

**Why `.cpu()`?**

Tensors on GPU (MPS/CUDA) must be moved to CPU before converting to NumPy:

```python
# On GPU
outputs.device  # mps:0 or cuda:0
outputs.numpy()  # ERROR! Can't convert GPU tensor to NumPy

# Move to CPU first
outputs_cpu = outputs.cpu()
outputs_cpu.device  # cpu
outputs_cpu.numpy()  # âœ“ Works!
```

NumPy only works with CPU tensors.

**Building prediction arrays:**

```python
# After all batches:
all_preds = [
    [0.85], [0.23], [0.91], ..., [0.34]  # From batch 1
    [0.12], [0.78], [0.45], ..., [0.89]  # From batch 2
    ...
]  # List of 8,567 predictions

all_labels = [
    [1.0], [0.0], [1.0], ..., [0.0]  # From batch 1
    [0.0], [1.0], [1.0], ..., [1.0]  # From batch 2
    ...
]  # List of 8,567 labels
```

### Computing Validation Metrics

```python
avg_val_loss = val_loss / len(val_loader)

# Convert to arrays
all_preds = np.array(all_preds)    # (8567, 1)
all_labels = np.array(all_labels)  # (8567, 1)

# Compute accuracy
val_acc = accuracy_score(
    all_labels > 0.5,  # True labels (binarized)
    all_preds > 0.5    # Predicted labels (threshold at 0.5)
)
```

**Accuracy calculation:**

```python
# Example
all_labels = [1, 0, 1, 1, 0]
all_preds = [0.92, 0.15, 0.78, 0.42, 0.08]

# Apply threshold
pred_binary = [1, 0, 1, 0, 0]  # 0.5 threshold

# Count matches
matches = [1==1, 0==0, 1==1, 1â‰ 0, 0==0]
        = [True, True, True, False, True]

accuracy = 4/5 = 0.80 = 80%
```

**Storing history:**

```python
self.training_history["train_loss"].append(avg_train_loss)
self.training_history["val_loss"].append(avg_val_loss)
self.training_history["val_acc"].append(val_acc)
```

This creates training curves:
```python
{
    "train_loss": [0.487, 0.412, 0.358, ..., 0.132],
    "val_loss": [0.451, 0.398, 0.345, ..., 0.146],
    "val_acc": [0.823, 0.854, 0.879, ..., 0.917]
}
```

### Learning Rate Scheduling

```python
scheduler.step(avg_val_loss)
```

Updates learning rate based on validation loss:

```python
# Internally:
if avg_val_loss < best_loss_so_far:
    best_loss = avg_val_loss
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        # Reduce learning rate
        new_lr = current_lr * factor
        for param_group in optimizer.param_groups:
            param_group['lr'] = new_lr
```

**Example learning rate schedule:**

```
Epoch | Val Loss | LR      | Action
------|----------|---------|---------------------------
1     | 0.487    | 0.002   | 
10    | 0.198    | 0.002   |
20    | 0.158    | 0.002   | (plateau detected)
21    | 0.157    | 0.001   | LR *= 0.5
30    | 0.148    | 0.001   | 
35    | 0.147    | 0.001   | (plateau again)
36    | 0.147    | 0.0005  | LR *= 0.5
42    | 0.146    | 0.0005  | (early stopping)
```

### Early Stopping

```python
if avg_val_loss < best_val_loss:
    best_val_loss = avg_val_loss
    patience_counter = 0
    best_state = self.model.state_dict().copy()
else:
    patience_counter += 1

if patience_counter >= self.patience:  # 15 epochs
    logger.info(f"Early stopping at epoch {epoch+1}")
    break
```

**Model state saving:**

```python
best_state = self.model.state_dict().copy()
```

**What is state_dict()?**

Dictionary of all model parameters:

```python
state_dict = {
    'input_bn.weight': tensor([...]),  # (10,)
    'input_bn.bias': tensor([...]),    # (10,)
    'hidden_layers.0.fc.weight': tensor([...]),  # (256, 10)
    'hidden_layers.0.fc.bias': tensor([...]),    # (256,)
    'hidden_layers.0.bn.weight': tensor([...]),  # (256,)
    ...
}
```

**Why `.copy()`?**

Without copy:
```python
best_state = model.state_dict()  # Reference
model.parameters_change()
# best_state also changed! (points to same memory)
```

With copy:
```python
best_state = model.state_dict().copy()  # Independent copy
model.parameters_change()
# best_state unchanged âœ“
```

### Restoring Best Model

```python
# After training loop
if best_state is not None:
    self.model.load_state_dict(best_state)
```

Restores parameters from epoch with best validation loss:

```
Epoch 25: Val loss = 0.146  â† Best
Epoch 26-40: Val loss increases
Epoch 40: Early stop

Load model from epoch 25 (best)
```

**Complete validation pseudocode:**

```python
for each validation batch:
    1. Move to device
    2. Forward pass (no backward)
    3. Compute loss
    4. Collect predictions

Compute metrics (accuracy)
Check if improved:
    If yes: Save model state
    If no: Increment patience
    If patience exceeded: Stop training

Restore best model
```

Time per validation epoch: ~10 seconds (34 batches)
Total time per epoch: 35s (train) + 10s (val) = 45 seconds

---

# PART 2: EVALUATION METRICS

## Lines 397-444: `evaluate()` Method

After training, we evaluate on test set:

```python
def evaluate(self, X, y):
    """
    Comprehensive evaluation on test/validation set.
    
    Computes 7 metrics:
    1. Accuracy
    2. Precision
    3. Recall
    4. F1 Score
    5. ROC-AUC
    6. PR-AUC (Precision-Recall AUC)
    7. Expected Calibration Error (ECE)
    
    Returns:
        metrics: Dict with all metric values
    """
    
    # Get predictions
    y_pred = self.predict(X)         # Binary predictions (0 or 1)
    y_proba = self.predict_proba(X)[:, 1]  # Probabilities [0, 1]
```

### Prediction Methods

```python
def predict(self, X):
    """Predict classes (0 or 1)."""
    probs = self.predict_proba(X)[:, 1]  # Get probability of class 1
    return (probs > 0.5).astype(int)     # Threshold at 0.5

def predict_proba(self, X):
    """Predict probabilities for both classes."""
    self.model.eval()
    X_scaled = self.scaler.transform(X)
    X_tensor = torch.FloatTensor(X_scaled).to(self.device)
    
    with torch.no_grad():
        probs = self.model(X_tensor).cpu().numpy()  # (N, 1)
    
    # Return (N, 2) array: [prob_class_0, prob_class_1]
    return np.hstack([1 - probs, probs])
```

**Example:**

```python
X_test: (8568, 10)  # Test features

# Predict probabilities
y_proba = model.predict_proba(X_test)
# Shape: (8568, 2)
# Example values:
# [[0.12, 0.88],  # 88% probability of class 1 (similar)
#  [0.93, 0.07],  # 7% probability of class 1 (not similar)
#  [0.45, 0.55],  # 55% probability of class 1 (borderline)
#  ...]

# Predict classes
y_pred = model.predict(X_test)
# Shape: (8568,)
# Example values:
# [1, 0, 1, ...]  # Binary predictions
```

### Metric 1: Accuracy

```python
metrics["accuracy"] = float(accuracy_score(y, y_pred))
```

**Formula:**

```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Where:
  TP = True Positives (predicted 1, actual 1)
  TN = True Negatives (predicted 0, actual 0)
  FP = False Positives (predicted 1, actual 0)
  FN = False Negatives (predicted 0, actual 1)
```

**Example:**

```
Confusion Matrix:
                Predicted
                0      1
Actual  0     3987    297    (TN=3987, FP=297)
        1      414   3870    (FN=414, TP=3870)

Accuracy = (3987 + 3870) / (3987 + 297 + 414 + 3870)
         = 7857 / 8568
         = 0.9170 = 91.70%
```

**Our test result: 91.73%**

**Interpretation:**
- 91.73% of predictions are correct
- 8.27% are wrong

**When accuracy is misleading:**

For imbalanced datasets:
```
Dataset: 95% class 0, 5% class 1

Dummy classifier (always predicts 0):
  Accuracy = 95%  â† Looks good but useless!
```

Our dataset is balanced (50/50), so accuracy is meaningful.

### Metric 2: Precision

```python
metrics["precision"] = float(precision_score(y, y_pred))
```

**Formula:**

```
Precision = TP / (TP + FP)
```

**Interpretation:**

"Of all patents we predicted as similar, what fraction actually were similar?"

**Example:**

```
TP = 3870  (correctly predicted similar)
FP = 297   (incorrectly predicted similar)

Precision = 3870 / (3870 + 297)
          = 3870 / 4167
          = 0.9287 = 92.87%
```

**Our test result: 92.87%**

**When precision matters:**

- High cost of false positives
- Example: Patent examiner focuses on "similar" patents
  - False positive: Wastes examiner time reviewing unrelated patent
  - Want high precision to minimize wasted time

**Trade-off:**

High precision often means missing some true positives (lower recall).

### Metric 3: Recall

```python
metrics["recall"] = float(recall_score(y, y_pred))
```

**Formula:**

```
Recall = TP / (TP + FN)
```

**Interpretation:**

"Of all actually similar patents, what fraction did we find?"

**Example:**

```
TP = 3870  (found these similar patents)
FN = 414   (missed these similar patents)

Recall = 3870 / (3870 + 414)
       = 3870 / 4284
       = 0.9033 = 90.33%
```

**Our test result: 90.33%**

**When recall matters:**

- High cost of false negatives
- Example: Prior art search
  - False negative: Miss a relevant patent â†’ Application wrongly approved
  - Want high recall to find ALL relevant prior art

**Trade-off:**

High recall often means accepting more false positives (lower precision).

### Metric 4: F1 Score

```python
metrics["f1"] = float(f1_score(y, y_pred))
```

**Formula:**

```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

Harmonic mean of precision and recall.

**Example:**

```
Precision = 0.9287
Recall = 0.9033

F1 = 2 Ã— (0.9287 Ã— 0.9033) / (0.9287 + 0.9033)
   = 2 Ã— 0.8390 / 1.8320
   = 1.6780 / 1.8320
   = 0.9159 = 91.59%
```

**Our test result: 91.59%**

**Why harmonic mean instead of arithmetic mean?**

Arithmetic mean:
```
(Precision + Recall) / 2 = (0.9287 + 0.9033) / 2 = 0.9160
```

Harmonic mean:
```
F1 = 0.9159
```

Very similar for balanced values. But for imbalanced:

```
Precision = 0.99, Recall = 0.10

Arithmetic: (0.99 + 0.10) / 2 = 0.545
Harmonic: 2Ã—(0.99Ã—0.10)/(0.99+0.10) = 0.182

Harmonic mean penalizes imbalance!
```

F1 forces both precision AND recall to be high.

### Metric 5: ROC-AUC

```python
metrics["roc_auc"] = float(roc_auc_score(y, y_proba))
```

**What is ROC-AUC?**

ROC = Receiver Operating Characteristic curve
AUC = Area Under the Curve

**How to construct ROC curve:**

1. Try all possible thresholds (0.0, 0.01, 0.02, ..., 1.0)
2. For each threshold, compute:
   - TPR (True Positive Rate) = TP / (TP + FN) = Recall
   - FPR (False Positive Rate) = FP / (FP + TN)
3. Plot TPR vs FPR
4. Compute area under curve

**Example:**

```
Threshold | TP   | FP   | FN   | TN   | TPR   | FPR
----------|------|------|------|------|-------|-------
0.0       | 4284 | 4284 | 0    | 0    | 1.000 | 1.000 (predict all as 1)
0.1       | 4250 | 1200 | 34   | 3084 | 0.992 | 0.280
0.3       | 4100 | 600  | 184  | 3684 | 0.957 | 0.140
0.5       | 3870 | 297  | 414  | 3987 | 0.903 | 0.069
0.7       | 3500 | 120  | 784  | 4164 | 0.817 | 0.028
0.9       | 2800 | 30   | 1484 | 4254 | 0.654 | 0.007
1.0       | 0    | 0    | 4284 | 4284 | 0.000 | 0.000 (predict all as 0)

ROC Curve: Plot these (FPR, TPR) points
AUC: Area under the curve
```

**Perfect classifier:**
- AUC = 1.0
- ROC curve goes straight up then right (TPR=1 while FPR=0)

**Random classifier:**
- AUC = 0.5
- ROC curve is diagonal line (TPR = FPR)

**Our result: AUC = 0.9720 (97.20%)**

**Interpretation:**

"If we pick a random similar pair and a random dissimilar pair, the model will rank the similar pair higher 97.20% of the time."

**Why ROC-AUC is great:**

1. **Threshold-independent:** Measures ranking quality, not specific threshold
2. **Handles imbalance:** Works even with imbalanced datasets
3. **Interpretable:** Single number summarizing performance across all thresholds

**Comparison:**

```
Metric      | Value  | Interpretation
------------|--------|--------------------------------------
Accuracy    | 91.73% | % of correct predictions at threshold 0.5
F1          | 91.59% | Balance of precision/recall at threshold 0.5
ROC-AUC     | 97.20% | Ranking quality across ALL thresholds â† Most informative
```

### Metric 6: PR-AUC (Precision-Recall AUC)

```python
if len(np.unique(y)) > 1:
    precision_vals, recall_vals, _ = precision_recall_curve(y, y_proba)
    metrics['pr_auc'] = float(auc(recall_vals, precision_vals))
```

**What is Precision-Recall curve?**

Similar to ROC, but plot Precision vs Recall:

```
Threshold | Precision | Recall
----------|-----------|--------
0.0       | 0.500     | 1.000  (predict all as 1)
0.3       | 0.872     | 0.957
0.5       | 0.929     | 0.903
0.7       | 0.967     | 0.817
1.0       | 1.000     | 0.000  (predict all as 0)

PR Curve: Plot (Recall, Precision)
PR-AUC: Area under curve
```

**Our result: PR-AUC = 0.975**

**When to use PR-AUC instead of ROC-AUC:**

PR-AUC is better for imbalanced datasets:

```
Dataset: 5% positive, 95% negative

Dummy classifier (always predict positive):
  ROC-AUC: 0.50 (looks random)
  PR-AUC: 0.05 (shows it's bad)

Good classifier:
  ROC-AUC: 0.98
  PR-AUC: 0.85
```

PR-AUC is more sensitive to performance on minority class.

For our balanced dataset, both metrics are informative.

### Metric 7: Expected Calibration Error (ECE)

```python
# Calibration: Are predicted probabilities accurate?
n_bins = 10
bin_edges = np.linspace(0, 1, n_bins + 1)
ece_total = 0.0
total_count = 0

for i in range(n_bins):
    lo, hi = bin_edges[i], bin_edges[i + 1]
    mask = (y_proba >= lo) & (y_proba < hi)
    
    if mask.sum() == 0:
        continue
    
    # Average predicted probability in this bin
    p_hat = float(y_proba[mask].mean())
    
    # Actual fraction of positives in this bin
    p_true = float(y[mask].mean())
    
    # Error
    count = int(mask.sum())
    ece_total += count * abs(p_hat - p_true)
    total_count += count

metrics['ece'] = float(ece_total / total_count if total_count else 0.0)
```

**What is calibration?**

A model is well-calibrated if:
```
When it predicts 70% probability â†’ 70% of those examples are actually positive
When it predicts 30% probability â†’ 30% of those examples are actually positive
```

**ECE calculation example:**

```
Bin [0.0, 0.1]: 500 examples
  Average predicted prob: 0.05
  Actual positive rate: 0.03
  Error: |0.05 - 0.03| Ã— 500 = 10

Bin [0.1, 0.2]: 800 examples
  Average predicted prob: 0.15
  Actual positive rate: 0.14
  Error: |0.15 - 0.14| Ã— 800 = 8

...

Bin [0.9, 1.0]: 600 examples
  Average predicted prob: 0.95
  Actual positive rate: 0.97
  Error: |0.95 - 0.97| Ã— 600 = 12

ECE = (10 + 8 + ... + 12) / (500 + 800 + ... + 600)
    = 286 / 8568
    = 0.0334 = 3.34%
```

**Our result: ECE = 0.028 (2.8%)**

**Interpretation:**

On average, predicted probabilities are within 2.8% of true frequencies.

**Well-calibrated example:**

```
Model predicts: [0.9, 0.8, 0.85, 0.82, 0.91]
Actual labels: [1, 1, 1, 0, 1]
True rate: 4/5 = 0.80
Average prediction: 0.856
Error: |0.856 - 0.80| = 0.056
```

**Poorly calibrated example:**

```
Model predicts: [0.99, 0.98, 0.99, 0.97, 0.99]
Actual labels: [1, 0, 1, 0, 1]
True rate: 3/5 = 0.60
Average prediction: 0.984
Error: |0.984 - 0.60| = 0.384  â† Large!
```

**Why calibration matters:**

For patent novelty:
- Predict 0.95 â†’ Examiner assumes 95% confidence â†’ Makes decisions accordingly
- If actual rate is only 70% â†’ Examiner misled!

Good calibration means predictions are trustworthy probabilities.

**Our model is well-calibrated (ECE=2.8%) thanks to:**
1. Sigmoid output (produces valid probabilities)
2. Binary cross-entropy loss (proper scoring rule)
3. Sufficient training data
4. Regularization (prevents overconfidence)

### Complete Test Results

```python
{
    "accuracy": 0.9173,      # 91.73% correct
    "precision": 0.9287,     # 92.87% precision
    "recall": 0.9033,        # 90.33% recall
    "f1": 0.9159,            # 91.59% F1 score
    "roc_auc": 0.9720,       # 97.20% ROC-AUC  â† Best metric
    "pr_auc": 0.9751,        # 97.51% PR-AUC
    "ece": 0.0280,           # 2.80% calibration error
    "brier_score": 0.0652,   # Brier score (lower is better)
    "confusion_matrix": [
        [3987, 297],         # TN=3987, FP=297
        [414, 3870]          # FN=414, TP=3870
    ]
}
```

**Why 97.2% ROC-AUC is excellent:**

State-of-the-art for similar tasks:
- Text classification: 85-92% ROC-AUC
- Patent classification: 80-88% ROC-AUC
- Our model: 97.2% ROC-AUC â† Exceptional!

This validates our approach:
- High-quality features (10 engineered features)
- Effective architecture (residual blocks)
- Proper regularization (dropout, weight decay, batch norm)
- Sufficient data (57K training pairs)

---

# PART 3: INFERENCE PIPELINE

## File: `src/app/patent_analyzer.py`

This is the main orchestrator for inference. When a user submits a patent, this coordinates the end-to-end pipeline.

### Architecture: Hybrid RAG

**Traditional RAG (Retrieval-Augmented Generation):**
```
User query â†’ Embed â†’ Search database â†’ Generate answer
```

**Our Hybrid RAG:**
```
User query â†’ Embed â†’ Search local (200K) + Search online (millions)
           â†“           â†“                    â†“
      LLM Keywords â†’ Merge results â†’ Score with ML â†’ LLM Explanation
```

Components:
1. **Local search**: PatentSBERTa embeddings, cosine similarity (fast, offline)
2. **Online search**: Google Patents via SerpAPI (comprehensive, requires API)
3. **LLM keywords**: Phi-3 generates search terms (smart queries)
4. **ML scoring**: PyTorch model scores similarity (accurate)
5. **LLM explanation**: Phi-3 generates human-readable report

### Initialization

```python
class PatentAnalyzer:
    def __init__(
        self,
        patents_path='data/sampled/patents_sampled.jsonl',
        embeddings_path='data/embeddings/patent_embeddings.npy',
        patent_ids_path='data/embeddings/patent_ids.json',
        use_full_phi3=False,
        use_online_search=True,
        use_llm_keywords=True,
        serpapi_key=None
    ):
        # Components (loaded lazily)
        self.embeddings = None
        self.patent_ids = None
        self.st_model = None  # PatentSBERTa
        self.pytorch_model = None  # Our trained classifier
        self.feature_extractor = None
        self.explainer = None  # Phi-3
        self.keyword_extractor = None  # LLM keyword generation
        self.online_searcher = None  # SerpAPI
```

**Lazy loading:**

We don't load everything at initialization:
```python
# __init__: Just set paths
self.embeddings = None  # Not loaded yet

# load(): Actually load when needed
def load(self):
    if self.embeddings is None:
        self.embeddings = np.load(...)  # Load now
```

**Why lazy loading?**

1. **Faster startup**: Streamlit app loads in 2 seconds instead of 30 seconds
2. **Conditional loading**: Only load online search if API key provided
3. **Memory efficiency**: Don't load features we won't use

### Main Analysis Method

```python
def analyze(
    self,
    query_text: str,
    top_k: int = 15,
    status_callback=None
) -> AnalysisResult:
    """
    Complete patent novelty assessment.
    
    Pipeline:
    1. Generate query embedding (2-3s)
    2. Local search - cosine similarity (1-2s)
    3. LLM keyword extraction (10-15s)
    4. Online search - SerpAPI (15-25s)
    5. Merge results (0.1s)
    6. Extract features (2-3s)
    7. Score with PyTorch model (0.5s)
    8. Rank by score (0.1s)
    9. Generate LLM explanation (30-45s)
    10. Return report (0.1s)
    
    Total: 60-90 seconds
    """
```

### Step 1: Generate Query Embedding

```python
# Step 1: Embed user's patent
if status_callback:
    status_callback("ðŸ”„ Generating embeddings...")

query_embedding = self.st_model.encode(query_text)
# Returns: (768,) numpy array
```

**What happens:**

User input:
```
"A wireless charging system using magnetic resonance coupling.
The system includes a transmitter coil and receiver coil..."
```

PatentSBERTa processing:
1. Tokenize: ["A", "wireless", "charging", "system", ...]
2. Add special tokens: [[CLS], "A", "wireless", ..., [SEP]]
3. Convert to IDs: [101, 2019, 2976, 4485, 2291, ...]
4. Pad to 512 tokens
5. Run through 12 transformer layers
6. Extract [CLS] representation
7. Output: 768-dimensional vector

```python
query_embedding = array([0.234, -0.567, 0.123, ..., 0.891])
# Shape: (768,)
```

**Time: 2-3 seconds** (transformer inference)

### Step 2: Local Search

```python
# Step 2: Search local database
if status_callback:
    status_callback("ðŸ” Searching local database (200K patents)...")

local_results = self._find_similar(
    query_embedding,
    top_k=top_k,
    status_callback=status_callback
)
```

Let's look at `_find_similar`:

```python
def _find_similar(
    self,
    query_embedding: np.ndarray,  # (768,)
    top_k: int = 15,
    status_callback=None
) -> List[Dict]:
    """Find similar patents using cosine similarity."""
    
    # Normalize query embedding
    query_norm = query_embedding / np.linalg.norm(query_embedding)
    # Now ||query_norm|| = 1
    
    # Normalize all database embeddings
    all_norms = self.embeddings / np.linalg.norm(
        self.embeddings, axis=1, keepdims=True
    )
    # Shape: (200000, 768), each row has norm 1
    
    # Compute cosine similarities
    similarities = np.dot(all_norms, query_norm)
    # Shape: (200000,)
    # For normalized vectors: dot product = cosine similarity
    
    # Get top-k indices
    top_indices = np.argsort(similarities)[::-1][:top_k]
    # argsort: indices that would sort array
    # [::-1]: reverse (highest first)
    # [:top_k]: take top k
```

**Detailed example:**

```python
# Query embedding (normalized)
query_norm = [0.1, 0.2, 0.3, ..., 0.05]  # 768 dims, ||.|| = 1

# Database embeddings (200K Ã— 768)
all_norms = [
    [0.15, 0.18, 0.32, ..., 0.07],  # Patent 0
    [0.08, 0.22, 0.28, ..., 0.04],  # Patent 1
    ...
    [0.11, 0.19, 0.31, ..., 0.06]   # Patent 199999
]

# Dot products (cosine similarities)
similarities = [
    0.87,  # Patent 0 very similar
    0.23,  # Patent 1 different
    0.15,  # Patent 2 very different
    ...
    0.91,  # Patent 15789 highly similar
    ...
]

# Sort
sorted_indices = argsort([0.87, 0.23, ..., 0.91, ...])
                = [15789, 0, ..., 1, 2]  # Descending order

# Top 15
top_15_indices = [15789, 0, 92341, ...]
```

**Load patent metadata:**

```python
results = []
for idx in top_indices:
    pid = self.patent_ids[idx]  # Get patent ID
    patent_data = self._load_patent(str(pid)) or {}  # Load from JSONL
    
    results.append({
        'patent_id': pid,
        'similarity': float(similarities[idx]),
        'title': patent_data.get('title', 'N/A'),
        'abstract': patent_data.get('abstract', 'N/A'),
        'year': patent_data.get('year', 'N/A'),
        'claims': patent_data.get('claims', []),
        'source': 'local'
    })

return results
```

**Time: 1-2 seconds**
- Normalization: 0.1s
- Dot product (200K Ã— 768): 0.8s (optimized BLAS)
- Sorting: 0.2s
- Loading metadata: 0.5s

---

(Continue with steps 3-9 in next message due to length...)

### Summary So Far

We've covered:
âœ“ Complete validation loop
âœ“ All 7 evaluation metrics in detail
âœ“ Inference pipeline initialization
âœ“ Embedding generation
âœ“ Local cosine similarity search

Still to cover:
- LLM keyword extraction
- Online search (SerpAPI)
- Result merging
- Feature extraction for pairs
- PyTorch scoring
- Ranking and novelty calculation
- LLM explanation generation

This comprehensive walkthrough shows every detail from data preprocessing through model training to inference!

