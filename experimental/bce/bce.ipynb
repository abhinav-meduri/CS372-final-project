{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weighted BCE Training (Experimental)\n",
        "\n",
        "1. Experimental weighted-BCE training for the PyTorch novelty model.\n",
        "2. Uses the same architecture as production (PatentNoveltyNet).\n",
        "3. Adds class weighting via BCEWithLogitsLoss (pos_weight).\n",
        "4. Saves model and metrics to experimental/bce/outputs/weighted_bce/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path().absolute().parent.parent))\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from src.models.pytorch_classifier import PatentNoveltyNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = Path().absolute().parent.parent\n",
        "X = np.load(root / \"data\" / \"features\" / \"train_features_v2.X.npy\")\n",
        "y = np.load(root / \"data\" / \"features\" / \"train_features_v2.y.npy\")\n",
        "\n",
        "if X.shape[1] == 13:\n",
        "    idx_keep = [i for i in range(13) if i not in [0, 1, 6]]\n",
        "    X = X[:, idx_keep]\n",
        "\n",
        "print(f\"Loaded data: {X.shape[0]} samples, {X.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Data Loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "train_ds = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train).unsqueeze(1))\n",
        "val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val).unsqueeze(1))\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            probs = model(Xb).cpu().numpy()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(yb.cpu().numpy())\n",
        "    probs = np.vstack(all_probs)[:, 0]\n",
        "    labels = np.vstack(all_labels)[:, 0]\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"precision\": precision_score(labels, preds, zero_division=0),\n",
        "        \"recall\": recall_score(labels, preds, zero_division=0),\n",
        "        \"f1\": f1_score(labels, preds, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(labels, probs),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "pos_weight = 2.0\n",
        "hidden_dims = [256, 128]\n",
        "dropout = 0.3\n",
        "lr = 0.002\n",
        "weight_decay = 1e-5\n",
        "max_epochs = 40\n",
        "patience = 6\n",
        "\n",
        "model = PatentNoveltyNet(input_dim=X.shape[1], hidden_dims=hidden_dims, dropout=dropout, use_residual=True).to(device)\n",
        "\n",
        "def forward_logits(x):\n",
        "    out = model.input_bn(x)\n",
        "    out = model.hidden_layers(out)\n",
        "    out = model.output_bn(out)\n",
        "    return model.output_layer(out)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "best_state = None\n",
        "patience_ctr = 0\n",
        "history = {\"train_loss\": [], \"val_loss\": [], \"val_metrics\": []}\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    batches = 0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = forward_logits(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        batches += 1\n",
        "    avg_train = train_loss / batches\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    v_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = forward_logits(Xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            val_loss += loss.item()\n",
        "            v_batches += 1\n",
        "    avg_val = val_loss / v_batches if v_batches else 0.0\n",
        "    scheduler.step(avg_val)\n",
        "\n",
        "    history[\"train_loss\"].append(avg_train)\n",
        "    history[\"val_loss\"].append(avg_val)\n",
        "\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    history[\"val_metrics\"].append(val_metrics)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}, Val Acc={val_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    if avg_val < best_val_loss:\n",
        "        best_val_loss = avg_val\n",
        "        best_state = model.state_dict()\n",
        "        patience_ctr = 0\n",
        "    else:\n",
        "        patience_ctr += 1\n",
        "        if patience_ctr >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "if best_state:\n",
        "    model.load_state_dict(best_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_dir = Path(\"experimental/bce/outputs/weighted_bce\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save({\"model_state_dict\": model.state_dict()}, out_dir / \"model.pt\")\n",
        "with open(out_dir / \"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "with open(out_dir / \"history.json\", \"w\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"Saved to {out_dir}\")\n",
        "print(f\"Final validation metrics: {history['val_metrics'][-1]}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
