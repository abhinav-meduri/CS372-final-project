{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Model - Complete Implementation\n",
        "\n",
        "Complete ensemble model implementation combining MLP, PyTorch NN, and Gradient Boosting with probability calibration using StackingClassifier and CalibratedClassifierCV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path().absolute().parent))\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, brier_score_loss, log_loss\n",
        "\n",
        "# Note: This notebook assumes MLP and PyTorch models are already trained and saved\n",
        "# The model class definitions are in train_mlp.ipynb and train_pytorch.ipynb\n",
        "# For a fully self-contained version, copy those class definitions here or import from the .py files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_dir = Path(\"data/features\")\n",
        "\n",
        "X_train = np.load(features_dir / \"train_features_v2.X.npy\")\n",
        "y_train = np.load(features_dir / \"train_features_v2.y.npy\")\n",
        "X_val = np.load(features_dir / \"val_features_v2.X.npy\")\n",
        "y_val = np.load(features_dir / \"val_features_v2.y.npy\")\n",
        "X_test = np.load(features_dir / \"test_features_v2.X.npy\")\n",
        "y_test = np.load(features_dir / \"test_features_v2.y.npy\")\n",
        "\n",
        "with open(features_dir / \"feature_names_v2.json\", \"r\") as f:\n",
        "    feature_names = json.load(f)\n",
        "\n",
        "print(f\"Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Base Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_model = PatentNoveltyClassifier.load('models')\n",
        "pytorch_model = PyTorchPatentClassifier.load('models/pytorch_nn')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Base Model Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mlp_proba_train = mlp_model.predict_proba(X_train)[:, 1]\n",
        "pytorch_proba_train = pytorch_model.predict_proba(X_train)[:, 1]\n",
        "\n",
        "mlp_proba_val = mlp_model.predict_proba(X_val)[:, 1]\n",
        "pytorch_proba_val = pytorch_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "mlp_proba_test = mlp_model.predict_proba(X_test)[:, 1]\n",
        "pytorch_proba_test = pytorch_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "X_meta_train = np.column_stack([mlp_proba_train, pytorch_proba_train, X_train_scaled])\n",
        "X_meta_val = np.column_stack([mlp_proba_val, pytorch_proba_val, X_val_scaled])\n",
        "X_meta_test = np.column_stack([mlp_proba_test, pytorch_proba_test, X_test_scaled])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Stacking Ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_models = [\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42))\n",
        "]\n",
        "\n",
        "meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "stacking = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacking.fit(X_meta_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibrate Probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "calibrated_ensemble = CalibratedClassifierCV(\n",
        "    stacking,\n",
        "    method='sigmoid',\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "calibrated_ensemble.fit(X_meta_val, y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Ensemble\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_test = calibrated_ensemble.predict(X_meta_test)\n",
        "y_proba_test = calibrated_ensemble.predict_proba(X_meta_test)[:, 1]\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_test),\n",
        "    'precision': precision_score(y_test, y_pred_test),\n",
        "    'recall': recall_score(y_test, y_pred_test),\n",
        "    'f1': f1_score(y_test, y_pred_test),\n",
        "    'roc_auc': roc_auc_score(y_test, y_proba_test),\n",
        "    'brier_score': brier_score_loss(y_test, y_proba_test),\n",
        "    'log_loss': log_loss(y_test, y_proba_test)\n",
        "}\n",
        "\n",
        "print(\"Test Metrics:\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensemble_dir = Path(\"models/ensemble\")\n",
        "ensemble_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(ensemble_dir / \"ensemble_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(calibrated_ensemble, f)\n",
        "\n",
        "with open(ensemble_dir / \"scaler.pkl\", \"wb\") as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "results_dir = Path(\"results/ensemble\")\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(results_dir / \"ensemble_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"Ensemble model saved to: {ensemble_dir}\")\n",
        "print(f\"Metrics saved to: {results_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
