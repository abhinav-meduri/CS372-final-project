{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier - Complete Implementation\n",
    "\n",
    "Complete MLP classifier implementation including model class definition, training, evaluation, and saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentNoveltyClassifier:\n",
    "    \"\"\"\n",
    "    MLP for binary classification with:\n",
    "    - Single hidden layer with ReLU activation\n",
    "    - Output layer with sigmoid activation\n",
    "    - Cross-entropy loss function\n",
    "    - Minibatch SGD optimization via sklearn\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_sizes : tuple of int, default=(64,)\n",
    "        Number of hidden units in each layer\n",
    "    alpha : float, default=1e-5\n",
    "        L2 regularization parameter\n",
    "    learning_rate_init : float, default=0.005\n",
    "        Initial learning rate\n",
    "    max_iter : int, default=500\n",
    "        Maximum number of iterations\n",
    "    early_stopping : bool, default=True\n",
    "        Whether to use early stopping\n",
    "    n_iter_no_change : int, default=20\n",
    "        Number of iterations with no improvement before stopping\n",
    "    random_state : int, default=42\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(64,),\n",
    "        alpha=1e-5,\n",
    "        learning_rate_init=0.005,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=20,\n",
    "        random_state=42\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            max_iter=max_iter,\n",
    "            early_stopping=early_stopping,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            random_state=random_state,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.training_history = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_names=None):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_scaled = self.scaler.transform(X_val)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            if hasattr(self.model, 'loss_curve_'):\n",
    "                self.training_history = {\n",
    "                    'loss': self.model.loss_curve_,\n",
    "                    'n_iter': self.model.n_iter_\n",
    "                }\n",
    "        else:\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            if hasattr(self.model, 'loss_curve_'):\n",
    "                self.training_history = {\n",
    "                    'loss': self.model.loss_curve_,\n",
    "                    'n_iter': self.model.n_iter_\n",
    "                }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for each row in X\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities for each row in X for each class\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y, y_proba) if len(np.unique(y)) > 1 else 0.0\n",
    "        }\n",
    "        \n",
    "        metrics['brier_score'] = np.mean((y_proba - y) ** 2)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_feature_importance(self, n_samples=1000):\n",
    "        if self.feature_names is None:\n",
    "            return {}\n",
    "        \n",
    "        importance = {}\n",
    "        \n",
    "        if hasattr(self.model, 'coefs_'):\n",
    "            first_layer_weights = np.abs(self.model.coefs_[0])\n",
    "            feature_importance = np.mean(first_layer_weights, axis=1)\n",
    "            \n",
    "            total = np.sum(feature_importance)\n",
    "            if total > 0:\n",
    "                feature_importance = feature_importance / total\n",
    "            \n",
    "            for i, name in enumerate(self.feature_names):\n",
    "                if i < len(feature_importance):\n",
    "                    importance[name] = float(feature_importance[i])\n",
    "                else:\n",
    "                    importance[name] = 0.0\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    def plot_training_curve(self, output_path=None):\n",
    "        if self.training_history is None or 'loss' not in self.training_history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_history['loss'], label='Training Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_roc_curve(self, X, y, output_path=None):\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "        auc = roc_auc_score(y, y_proba)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})', linewidth=2)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, X, y, output_path=None):\n",
    "        y_pred = self.predict(X)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, models_dir: str):\n",
    "        models_dir = Path(models_dir)\n",
    "        models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        mlp_dir = models_dir / 'mlp'\n",
    "        mlp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(mlp_dir / 'mlp_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        \n",
    "        with open(mlp_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        metadata = {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'alpha': self.alpha,\n",
    "            'learning_rate_init': self.learning_rate_init,\n",
    "            'max_iter': self.max_iter,\n",
    "            'early_stopping': self.early_stopping,\n",
    "            'n_iter_no_change': self.n_iter_no_change,\n",
    "            'random_state': self.random_state,\n",
    "            'feature_names': self.feature_names,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "        \n",
    "        with open(mlp_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, models_dir: str):\n",
    "        models_dir = Path(models_dir)\n",
    "        mlp_dir = models_dir / 'mlp'\n",
    "        \n",
    "        if not mlp_dir.exists():\n",
    "            raise FileNotFoundError(f\"MLP model directory not found: {mlp_dir}\")\n",
    "        \n",
    "        with open(mlp_dir / 'metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        instance = cls(\n",
    "            hidden_layer_sizes=tuple(metadata['hidden_layer_sizes']),\n",
    "            alpha=metadata['alpha'],\n",
    "            learning_rate_init=metadata['learning_rate_init'],\n",
    "            max_iter=metadata['max_iter'],\n",
    "            early_stopping=metadata['early_stopping'],\n",
    "            n_iter_no_change=metadata['n_iter_no_change'],\n",
    "            random_state=metadata['random_state']\n",
    "        )\n",
    "        \n",
    "        with open(mlp_dir / 'mlp_model.pkl', 'rb') as f:\n",
    "            instance.model = pickle.load(f)\n",
    "        \n",
    "        with open(mlp_dir / 'scaler.pkl', 'rb') as f:\n",
    "            instance.scaler = pickle.load(f)\n",
    "        \n",
    "        instance.feature_names = metadata.get('feature_names')\n",
    "        instance.training_history = metadata.get('training_history')\n",
    "        \n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train: 39979 samples, 12 features\n",
      "Loaded val: 8567 samples, 12 features\n",
      "Loaded test: 8568 samples, 12 features\n",
      "\n",
      "Feature names: ['bm25_doc_score', 'bm25_best_claim_score', 'cosine_doc_similarity', 'cosine_max_claim_similarity', 'embedding_diff_mean', 'embedding_diff_std', 'cpc_jaccard', 'year_diff', 'title_jaccard', 'abstract_length_ratio', 'claim_count_ratio', 'shared_rare_terms_ratio']\n",
      "Training set: 39979 samples\n",
      "Validation set: 8567 samples\n",
      "Test set: 8568 samples"
     ]
    }
   ],
   "source": [
    "features_dir = Path('data/features')\n",
    "\n",
    "data = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    X = np.load(features_dir / f'{split}_features.X.npy')\n",
    "    y = np.load(features_dir / f'{split}_features.y.npy')\n",
    "    data[split] = {'X': X, 'y': y}\n",
    "    print(f\"Loaded {split}: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "with open(features_dir / 'feature_names.json', 'r') as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "X_train, y_train = data['train']['X'], data['train']['y']\n",
    "X_val, y_val = data['val']['X'], data['val']['y']\n",
    "X_test, y_test = data['test']['X'], data['test']['y']\n",
    "\n",
    "print(f\"\\nFeature names: {feature_names}\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: bm25_doc_score: zero variance (all 0.00)\n",
      "Warning: bm25_best_claim_score: zero variance (all 0.00)\n",
      "cosine_doc_similarity: mean=0.572, std=0.243\n",
      "cosine_max_claim_similarity: mean=0.572, std=0.243\n",
      "embedding_diff_mean: mean=0.024, std=0.010\n",
      "embedding_diff_std: mean=0.019, std=0.008\n",
      "Warning: cpc_jaccard: zero variance (all 0.00)\n",
      "year_diff: mean=0.181, std=0.118\n",
      "title_jaccard: mean=0.382, std=0.421\n",
      "abstract_length_ratio: mean=0.804, std=0.363\n",
      "claim_count_ratio: mean=0.678, std=0.328\n",
      "shared_rare_terms_ratio: mean=0.282, std=0.380"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(feature_names):\n",
    "    train_col = X_train[:, i]\n",
    "    if train_col.std() == 0:\n",
    "        print(f\"Warning: {name}: zero variance (all {train_col[0]:.2f})\")\n",
    "    else:\n",
    "        print(f\"{name}: mean={train_col.mean():.3f}, std={train_col.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PatentNoveltyClassifier(\n",
    "    hidden_layer_sizes=(64,),\n",
    "    alpha=1e-5,\n",
    "    learning_rate_init=0.005,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, X_val, y_val, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "  Accuracy:  0.8720\n",
      "  Precision: 0.8943\n",
      "  Recall:    0.8432\n",
      "  F1:        0.8680\n",
      "  ROC-AUC:   0.9406\n",
      "\n",
      "Validation Metrics:\n",
      "  Accuracy:  0.8760\n",
      "  Precision: 0.9013\n",
      "  Recall:    0.8486\n",
      "  F1:        0.8742\n",
      "  ROC-AUC:   0.9444\n",
      "\n",
      "Test Metrics:\n",
      "  Accuracy:  0.8758\n",
      "  Precision: 0.8928\n",
      "  Recall:    0.8516\n",
      "  F1:        0.8717\n",
      "  ROC-AUC:   0.9453\n",
      "  Brier:     0.0905"
     ]
    }
   ],
   "source": [
    "train_metrics = clf.evaluate(X_train, y_train)\n",
    "val_metrics = clf.evaluate(X_val, y_val)\n",
    "test_metrics = clf.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  Accuracy:  {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {train_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {train_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {train_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {val_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {val_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  Brier:     {test_metrics['brier_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: models\n",
      "Plots saved to: results/plots\n",
      "Metrics saved to: results/metrics"
     ]
    }
   ],
   "source": [
    "models_dir = Path('models')\n",
    "results_dir = Path('results')\n",
    "plots_dir = results_dir / 'plots'\n",
    "metrics_dir = results_dir / 'metrics'\n",
    "\n",
    "for d in [models_dir, plots_dir, metrics_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clf.save(models_dir)\n",
    "\n",
    "clf.plot_training_curve(plots_dir / 'training_curve.png')\n",
    "clf.plot_roc_curve(X_test, y_test, plots_dir / 'roc_curve.png')\n",
    "clf.plot_confusion_matrix(X_test, y_test, plots_dir / 'confusion_matrix.png')\n",
    "\n",
    "all_metrics = {\n",
    "    'train': train_metrics,\n",
    "    'val': val_metrics,\n",
    "    'test': test_metrics,\n",
    "    'feature_importance': importance\n",
    "}\n",
    "\n",
    "with open(metrics_dir / 'all_metrics.json', 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model saved to: {models_dir}\")\n",
    "print(f\"Plots saved to: {plots_dir}\")\n",
    "print(f\"Metrics saved to: {metrics_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
