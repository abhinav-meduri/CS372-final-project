{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Classifier - Complete Implementation\n",
    "\n",
    "Complete MLP classifier implementation including model class definition, training, evaluation, and saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentNoveltyClassifier:\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron classifier for patent novelty assessment.\n",
    "    \n",
    "    Wraps sklearn's MLPClassifier with additional features:\n",
    "    - Training history tracking\n",
    "    - Feature importance via permutation importance\n",
    "    - Model persistence\n",
    "    - Visualization utilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes: Tuple[int, ...] = (64,),\n",
    "        alpha: float = 1e-5,\n",
    "        learning_rate_init: float = 0.005,\n",
    "        max_iter: int = 500,\n",
    "        early_stopping: bool = True,\n",
    "        n_iter_no_change: int = 20,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            alpha=alpha,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            max_iter=max_iter,\n",
    "            early_stopping=early_stopping,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            random_state=random_state,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.training_history = None\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_val: Optional[np.ndarray] = None,\n",
    "        y_val: Optional[np.ndarray] = None,\n",
    "        feature_names: Optional[List[str]] = None\n",
    "    ):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_scaled = self.scaler.transform(X_val)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            if hasattr(self.model, 'loss_curve_'):\n",
    "                self.training_history = {\n",
    "                    'loss': self.model.loss_curve_,\n",
    "                    'n_iter': self.model.n_iter_\n",
    "                }\n",
    "        else:\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            if hasattr(self.model, 'loss_curve_'):\n",
    "                self.training_history = {\n",
    "                    'loss': self.model.loss_curve_,\n",
    "                    'n_iter': self.model.n_iter_\n",
    "                }\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict:\n",
    "        y_pred = self.predict(X)\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y, y_proba) if len(np.unique(y)) > 1 else 0.0\n",
    "        }\n",
    "        \n",
    "        metrics['brier_score'] = np.mean((y_proba - y) ** 2)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_feature_importance(self, n_samples: int = 1000) -> Dict[str, float]:\n",
    "        if self.feature_names is None:\n",
    "            return {}\n",
    "        \n",
    "        importance = {}\n",
    "        \n",
    "        if hasattr(self.model, 'coefs_'):\n",
    "            first_layer_weights = np.abs(self.model.coefs_[0])\n",
    "            feature_importance = np.mean(first_layer_weights, axis=1)\n",
    "            \n",
    "            total = np.sum(feature_importance)\n",
    "            if total > 0:\n",
    "                feature_importance = feature_importance / total\n",
    "            \n",
    "            for i, name in enumerate(self.feature_names):\n",
    "                if i < len(feature_importance):\n",
    "                    importance[name] = float(feature_importance[i])\n",
    "                else:\n",
    "                    importance[name] = 0.0\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    def plot_training_curve(self, output_path: Optional[str] = None):\n",
    "        if self.training_history is None or 'loss' not in self.training_history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.training_history['loss'], label='Training Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_roc_curve(self, X: np.ndarray, y: np.ndarray, output_path: Optional[str] = None):\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "        auc = roc_auc_score(y, y_proba)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})', linewidth=2)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, X: np.ndarray, y: np.ndarray, output_path: Optional[str] = None):\n",
    "        y_pred = self.predict(X)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def save(self, models_dir: str):\n",
    "        models_dir = Path(models_dir)\n",
    "        models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        mlp_dir = models_dir / 'mlp'\n",
    "        mlp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(mlp_dir / 'mlp_model.pkl', 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        \n",
    "        with open(mlp_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        metadata = {\n",
    "            'hidden_layer_sizes': self.hidden_layer_sizes,\n",
    "            'alpha': self.alpha,\n",
    "            'learning_rate_init': self.learning_rate_init,\n",
    "            'max_iter': self.max_iter,\n",
    "            'early_stopping': self.early_stopping,\n",
    "            'n_iter_no_change': self.n_iter_no_change,\n",
    "            'random_state': self.random_state,\n",
    "            'feature_names': self.feature_names,\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "        \n",
    "        with open(mlp_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, models_dir: str):\n",
    "        models_dir = Path(models_dir)\n",
    "        mlp_dir = models_dir / 'mlp'\n",
    "        \n",
    "        if not mlp_dir.exists():\n",
    "            raise FileNotFoundError(f\"MLP model directory not found: {mlp_dir}\")\n",
    "        \n",
    "        with open(mlp_dir / 'metadata.json', 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        instance = cls(\n",
    "            hidden_layer_sizes=tuple(metadata['hidden_layer_sizes']),\n",
    "            alpha=metadata['alpha'],\n",
    "            learning_rate_init=metadata['learning_rate_init'],\n",
    "            max_iter=metadata['max_iter'],\n",
    "            early_stopping=metadata['early_stopping'],\n",
    "            n_iter_no_change=metadata['n_iter_no_change'],\n",
    "            random_state=metadata['random_state']\n",
    "        )\n",
    "        \n",
    "        with open(mlp_dir / 'mlp_model.pkl', 'rb') as f:\n",
    "            instance.model = pickle.load(f)\n",
    "        \n",
    "        with open(mlp_dir / 'scaler.pkl', 'rb') as f:\n",
    "            instance.scaler = pickle.load(f)\n",
    "        \n",
    "        instance.feature_names = metadata.get('feature_names')\n",
    "        instance.training_history = metadata.get('training_history')\n",
    "        \n",
    "        return instance\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = Path('data/features')\n",
    "\n",
    "data = {}\n",
    "for split in ['train', 'val', 'test']:\n",
    "    X = np.load(features_dir / f'{split}_features.X.npy')\n",
    "    y = np.load(features_dir / f'{split}_features.y.npy')\n",
    "    data[split] = {'X': X, 'y': y}\n",
    "    print(f\"Loaded {split}: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "with open(features_dir / 'feature_names.json', 'r') as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "X_train, y_train = data['train']['X'], data['train']['y']\n",
    "X_val, y_val = data['val']['X'], data['val']['y']\n",
    "X_test, y_test = data['test']['X'], data['test']['y']\n",
    "\n",
    "print(f\"\\nFeature names: {feature_names}\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(feature_names):\n",
    "    train_col = X_train[:, i]\n",
    "    if train_col.std() == 0:\n",
    "        print(f\"  Warning: {name}: zero variance (all {train_col[0]:.2f})\")\n",
    "    else:\n",
    "        print(f\"   {name}: mean={train_col.mean():.3f}, std={train_col.std():.3f}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PatentNoveltyClassifier(\n",
    "    hidden_layer_sizes=(64,),\n",
    "    alpha=1e-5,\n",
    "    learning_rate_init=0.005,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train, X_val, y_val, feature_names)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = clf.evaluate(X_train, y_train)\n",
    "val_metrics = clf.evaluate(X_val, y_val)\n",
    "test_metrics = clf.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  Accuracy:  {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {train_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {train_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {train_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {val_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {val_metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1:        {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  Brier:     {test_metrics['brier_score']:.4f}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf.get_feature_importance()\n",
    "for name, imp in sorted(importance.items(), key=lambda x: -x[1]):\n",
    "    bar = \"\u2588\" * int(imp * 50)\n",
    "    print(f\"  {name:30s} {imp:.4f} {bar}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path('models')\n",
    "results_dir = Path('results')\n",
    "plots_dir = results_dir / 'plots'\n",
    "metrics_dir = results_dir / 'metrics'\n",
    "\n",
    "for d in [models_dir, plots_dir, metrics_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clf.save(models_dir)\n",
    "\n",
    "clf.plot_training_curve(plots_dir / 'training_curve.png')\n",
    "clf.plot_roc_curve(X_test, y_test, plots_dir / 'roc_curve.png')\n",
    "clf.plot_confusion_matrix(X_test, y_test, plots_dir / 'confusion_matrix.png')\n",
    "\n",
    "all_metrics = {\n",
    "    'train': train_metrics,\n",
    "    'val': val_metrics,\n",
    "    'test': test_metrics,\n",
    "    'feature_importance': importance\n",
    "}\n",
    "\n",
    "with open(metrics_dir / 'all_metrics.json', 'w') as f:\n",
    "    json.dump(all_metrics, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model saved to: {models_dir}\")\n",
    "print(f\"Plots saved to: {plots_dir}\")\n",
    "print(f\"Metrics saved to: {metrics_dir}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}