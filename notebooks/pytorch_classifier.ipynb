{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Classifier (Currently Using)\n",
    "\n",
    "Complete PyTorch neural network implementation including model class definitions (ResidualBlock, PatentNoveltyNet, PyTorchPatentClassifier), training on 10 features, evaluation, and saving.\n",
    "\n",
    "Note: Most of the code in this notebook was adapted from CS 372 homework assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import Tuple, Dict, Optional, List\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, precision_recall_curve,\n",
    "    average_precision_score, auc, brier_score_loss\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(random_seed)\n",
    "    \n",
    "# Set environment variables for determinism\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with batch norm and dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, dropout=0.3, bn_momentum=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features, momentum=bn_momentum)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        if in_features != out_features:\n",
    "            self.skip = nn.Linear(in_features, out_features)\n",
    "            self.skip_bn = nn.BatchNorm1d(out_features, momentum=bn_momentum)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "            self.skip_bn = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        if self.skip_bn is not None:\n",
    "            identity = self.skip_bn(identity)\n",
    "        \n",
    "        out = self.fc(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + identity\n",
    "        return out\n",
    "\n",
    "\n",
    "class PatentNoveltyNet(nn.Module):\n",
    "    \"\"\"Neural network for novelty classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3, use_residual=True, bn_momentum=0.1):\n",
    "        super(PatentNoveltyNet, self).__init__()\n",
    "        \n",
    "        self.input_bn = nn.BatchNorm1d(input_dim, momentum=bn_momentum)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            if use_residual:\n",
    "                layers.append(ResidualBlock(prev_dim, hidden_dim, dropout, bn_momentum))\n",
    "            else:\n",
    "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim, momentum=bn_momentum))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_bn = nn.BatchNorm1d(prev_dim, momentum=bn_momentum)\n",
    "        self.output_layer = nn.Linear(prev_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.input_bn(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_bn(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PyTorchPatentClassifier:\n",
    "    \"\"\"PyTorch classifier for patent novelty prediction.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims=[128, 64, 32],\n",
    "        dropout=0.3,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=256,\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        use_residual=True,\n",
    "        bn_momentum=0.1,\n",
    "        device=None\n",
    "    ):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.use_residual = use_residual\n",
    "        self.bn_momentum = bn_momentum\n",
    "        \n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            elif torch.backends.mps.is_available():\n",
    "                self.device = torch.device(\"mps\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.training_history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "        self.feature_names = None\n",
    "        \n",
    "        logger.info(f\"Initialized (device: {self.device})\")\n",
    "    \n",
    "    def _create_dataloaders(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        X_train_t = torch.FloatTensor(X_train)\n",
    "        y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "        \n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(42)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            generator=generator,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        val_loader = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val_t = torch.FloatTensor(X_val)\n",
    "            y_val_t = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "            val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset, \n",
    "                batch_size=self.batch_size, \n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=False\n",
    "            )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, feature_names=None, use_mixup=True, mixup_alpha=0.2):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.scaler.transform(X_val) if X_val is not None else None\n",
    "        \n",
    "        input_dim = X_train.shape[1]\n",
    "        self.model = PatentNoveltyNet(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            dropout=self.dropout,\n",
    "            use_residual=self.use_residual,\n",
    "            bn_momentum=self.bn_momentum\n",
    "        ).to(self.device)\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        train_loader, val_loader = self._create_dataloaders(\n",
    "            X_train_scaled, y_train, X_val_scaled, y_val\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training: dims={self.hidden_dims}, dropout={self.dropout}, mixup={use_mixup}\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                if use_mixup and np.random.random() > 0.5:\n",
    "                    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "                    index = torch.randperm(batch_X.size(0)).to(self.device)\n",
    "                    batch_X = lam * batch_X + (1 - lam) * batch_X[index]\n",
    "                    batch_y = lam * batch_y + (1 - lam) * batch_y[index]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_train_loss = train_loss / num_batches if num_batches > 0 else 0.0\n",
    "            self.training_history[\"train_loss\"].append(avg_train_loss)\n",
    "            \n",
    "            if val_loader is not None:\n",
    "                self.model.eval()\n",
    "                val_loss = 0.0\n",
    "                all_preds = []\n",
    "                all_labels = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        batch_X = batch_X.to(self.device)\n",
    "                        batch_y = batch_y.to(self.device)\n",
    "                        \n",
    "                        outputs = self.model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "                        \n",
    "                        all_preds.extend(outputs.cpu().numpy())\n",
    "                        all_labels.extend(batch_y.cpu().numpy())\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                val_acc = accuracy_score(\n",
    "                    np.array(all_labels) > 0.5,\n",
    "                    np.array(all_preds) > 0.5\n",
    "                )\n",
    "                \n",
    "                self.training_history[\"val_loss\"].append(avg_val_loss)\n",
    "                self.training_history[\"val_acc\"].append(val_acc)\n",
    "                \n",
    "                scheduler.step(avg_val_loss)\n",
    "                \n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    best_state = self.model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    logger.info(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f}, Val={avg_val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "                \n",
    "                if patience_counter >= self.patience:\n",
    "                    logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        if best_state is not None:\n",
    "            self.model.load_state_dict(best_state)\n",
    "        \n",
    "        logger.info(\"Training complete\")\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        self.model.eval()\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = self.model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        return np.hstack([1 - probs, probs])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\"\"\"\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs > 0.5).astype(int)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y, y_pred),\n",
    "            \"precision\": precision_score(y, y_pred),\n",
    "            \"recall\": recall_score(y, y_pred),\n",
    "            \"f1\": f1_score(y, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y, y_proba),\n",
    "            \"confusion_matrix\": confusion_matrix(y, y_pred).tolist()\n",
    "        }\n",
    "        \n",
    "        if len(np.unique(y)) > 1:\n",
    "            precision_vals, recall_vals, _ = precision_recall_curve(y, y_proba)\n",
    "            metrics['pr_auc'] = auc(recall_vals, precision_vals)\n",
    "            metrics['average_precision'] = average_precision_score(y, y_proba)\n",
    "        else:\n",
    "            metrics['pr_auc'] = 0.0\n",
    "            metrics['average_precision'] = 0.0\n",
    "        \n",
    "        metrics['brier_score'] = brier_score_loss(y, y_proba)\n",
    "        \n",
    "        n_bins = 10\n",
    "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        ece_total = 0.0\n",
    "        total_count = 0\n",
    "        calibration_bins = []\n",
    "        for i in range(n_bins):\n",
    "            lo, hi = bin_edges[i], bin_edges[i + 1]\n",
    "            mask = (y_proba >= lo) & (y_proba < hi if i < n_bins - 1 else y_proba <= hi)\n",
    "            if mask.sum() == 0:\n",
    "                calibration_bins.append({\"bin\": i, \"range\": [float(lo), float(hi)], \"count\": 0})\n",
    "                continue\n",
    "            p_hat = float(y_proba[mask].mean())\n",
    "            p_true = float(y[mask].mean())\n",
    "            count = int(mask.sum())\n",
    "            calibration_bins.append({\n",
    "                \"bin\": i, \"range\": [float(lo), float(hi)], \"count\": count,\n",
    "                \"pred_mean\": p_hat, \"true_mean\": p_true\n",
    "            })\n",
    "            ece_total += count * abs(p_hat - p_true)\n",
    "            total_count += count\n",
    "        metrics['ece'] = float(ece_total / total_count if total_count else 0.0)\n",
    "        metrics['calibration_bins'] = calibration_bins\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save({\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"hidden_dims\": self.hidden_dims,\n",
    "            \"dropout\": self.dropout,\n",
    "            \"use_residual\": self.use_residual,\n",
    "            \"bn_momentum\": self.bn_momentum,\n",
    "            \"input_dim\": self.model.input_bn.num_features\n",
    "        }, path / \"pytorch_model.pt\")\n",
    "        \n",
    "        with open(path / \"scaler_pytorch.pkl\", \"wb\") as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(path / \"training_history_pytorch.json\", \"w\") as f:\n",
    "            json.dump(self.training_history, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        path = Path(path)\n",
    "        \n",
    "        checkpoint = torch.load(path / \"pytorch_model.pt\", map_location=self.device)\n",
    "        \n",
    "        self.hidden_dims = checkpoint[\"hidden_dims\"]\n",
    "        self.dropout = checkpoint[\"dropout\"]\n",
    "        self.use_residual = checkpoint[\"use_residual\"]\n",
    "        self.bn_momentum = checkpoint.get(\"bn_momentum\", 0.1)\n",
    "        \n",
    "        self.model = PatentNoveltyNet(\n",
    "            input_dim=checkpoint[\"input_dim\"],\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            dropout=self.dropout,\n",
    "            use_residual=self.use_residual,\n",
    "            bn_momentum=self.bn_momentum\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        \n",
    "        with open(path / \"scaler_pytorch.pkl\", \"rb\") as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 13\n",
      "Original feature names: ['bm25_doc_score', 'bm25_best_claim_score', 'cosine_doc_similarity', 'cosine_max_claim_similarity', 'embedding_diff_mean', 'embedding_diff_std', 'cpc_jaccard', 'year_diff', 'title_jaccard', 'abstract_length_ratio', 'claim_count_ratio', 'shared_rare_terms_ratio', 'claim_similarity']"
     ]
    }
   ],
   "source": [
    "features_dir = Path(\"data/features\")\n",
    "\n",
    "X_train = np.load(features_dir / \"train_features_v2.X.npy\")\n",
    "y_train = np.load(features_dir / \"train_features_v2.y.npy\")\n",
    "X_val = np.load(features_dir / \"val_features_v2.X.npy\")\n",
    "y_val = np.load(features_dir / \"val_features_v2.y.npy\")\n",
    "X_test = np.load(features_dir / \"test_features_v2.X.npy\")\n",
    "y_test = np.load(features_dir / \"test_features_v2.y.npy\")\n",
    "\n",
    "old_feature_names = [\n",
    "    'bm25_doc_score',\n",
    "    'bm25_best_claim_score',\n",
    "    'cosine_doc_similarity',\n",
    "    'cosine_max_claim_similarity',\n",
    "    'embedding_diff_mean',\n",
    "    'embedding_diff_std',\n",
    "    'cpc_jaccard',\n",
    "    'year_diff',\n",
    "    'title_jaccard',\n",
    "    'abstract_length_ratio',\n",
    "    'claim_count_ratio',\n",
    "    'shared_rare_terms_ratio',\n",
    "    'claim_similarity'\n",
    "]\n",
    "\n",
    "print(f\"Original features: {len(old_feature_names)}\")\n",
    "print(f\"Original feature names: {old_feature_names}\")\n",
    "\n",
    "assert X_train.shape[1] == 13, f\"Expected 13 features, got {X_train.shape[1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove BM25 and CPC Features (After Ablation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Removing features at indices [0, 1, 6]\n",
      "Removing: ['bm25_doc_score', 'bm25_best_claim_score', 'cpc_jaccard']\n",
      "Kept features: ['cosine_doc_similarity', 'cosine_max_claim_similarity', 'embedding_diff_mean', 'embedding_diff_std', 'year_diff', 'title_jaccard', 'abstract_length_ratio', 'claim_count_ratio', 'shared_rare_terms_ratio', 'claim_similarity']\n",
      "New feature count: 10\n",
      "Updated feature_names_v2.json with 10 features\n",
      "\n",
      "Training set: 39979 samples, 10 features\n",
      "Validation set: 8567 samples\n",
      "Test set: 8568 samples"
     ]
    }
   ],
   "source": [
    "indices_to_remove = [0, 1, 6]\n",
    "indices_to_keep = [i for i in range(13) if i not in indices_to_remove]\n",
    "\n",
    "print(f\"\\n2. Removing features at indices {indices_to_remove}\")\n",
    "print(f\"Removing: {[old_feature_names[i] for i in indices_to_remove]}\")\n",
    "\n",
    "X_train = X_train[:, indices_to_keep]\n",
    "X_val = X_val[:, indices_to_keep]\n",
    "X_test = X_test[:, indices_to_keep]\n",
    "\n",
    "feature_names = [old_feature_names[i] for i in indices_to_keep]\n",
    "\n",
    "print(f\"Kept features: {feature_names}\")\n",
    "print(f\"New feature count: {len(feature_names)}\")\n",
    "\n",
    "with open(features_dir / \"feature_names_v2.json\", \"w\") as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(f\"Updated feature_names_v2.json with 10 features\")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "assert X_train.shape[1] == 10, f\"Expected 10 features, got {X_train.shape[1]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchPatentClassifier(\n",
    "    hidden_dims=[256, 128],\n",
    "    dropout=0.3,\n",
    "    learning_rate=0.002,\n",
    "    weight_decay=1e-05,\n",
    "    max_epochs=100,\n",
    "    patience=15,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4. Training model\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    feature_names=feature_names,\n",
    "    use_mixup=False\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Evaluating model\n",
      "\n",
      "Results:\n",
      "\n",
      "Training set:\n",
      "Accuracy: 0.9151\n",
      "ROC-AUC: 0.9725\n",
      "F1: 0.9135\n",
      "\n",
      "Validation set:\n",
      "Accuracy: 0.9206\n",
      "ROC-AUC: 0.9748\n",
      "F1: 0.9206\n",
      "\n",
      "Test set:\n",
      "Accuracy: 0.9173\n",
      "ROC-AUC: 0.9720\n",
      "PR-AUC: 0.9747\n",
      "Average precision: 0.9747\n",
      "Brier: 0.0627\n",
      "ECE: 0.0089\n",
      "F1: 0.9153"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Evaluating model\")\n",
    "train_metrics = model.evaluate(X_train, y_train)\n",
    "val_metrics = model.evaluate(X_val, y_val)\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"ROC-AUC: {train_metrics['roc_auc']:.4f}\")\n",
    "print(f\"F1: {train_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "print(f\"F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "print(f\"Average precision: {test_metrics['average_precision']:.4f}\")\n",
    "print(f\"Brier: {test_metrics['brier_score']:.4f}\")\n",
    "print(f\"ECE: {test_metrics['ece']:.4f}\")\n",
    "print(f\"F1: {test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Saving Model\n",
      "Model saved to: models/pytorch_nn\n",
      "Results saved to: models/pytorch_nn/training_results.json\n",
      "\n",
      "Model retrained with 10 features (BM25 and CPC removed)\n",
      "Feature names updated in: data/features/feature_names_v2.json"
     ]
    }
   ],
   "source": [
    "print(\"\\n6. Saving Model\")\n",
    "model_path = Path(\"models/pytorch_nn\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "model.save(model_path)\n",
    "\n",
    "results = {\n",
    "    \"training_time_minutes\": training_time / 60,\n",
    "    \"features\": feature_names,\n",
    "    \"num_features\": 10,\n",
    "    \"removed_features\": [old_feature_names[i] for i in indices_to_remove],\n",
    "    \"train_metrics\": train_metrics,\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"test_metrics\": test_metrics,\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "results_path = model_path / \"training_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "print(f\"\\nModel retrained with 10 features (BM25 and CPC removed)\")\n",
    "print(f\"Feature names updated in: data/features/feature_names_v2.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
