{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Classifier (Currently Used)\n",
        "\n",
        "Complete PyTorch neural network implementation including model class definitions (ResidualBlock, PatentNoveltyNet, PyTorchPatentClassifier), training on 10 features, evaluation, and saving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path().absolute().parent))\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from typing import Tuple, Dict, Optional, List\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Class Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with batch norm and dropout.\"\"\"\n",
        "    \n",
        "    def __init__(self, in_features: int, out_features: int, dropout: float = 0.3, bn_momentum: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(in_features, out_features)\n",
        "        self.bn = nn.BatchNorm1d(out_features, momentum=bn_momentum)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "        if in_features != out_features:\n",
        "            self.skip = nn.Linear(in_features, out_features)\n",
        "            self.skip_bn = nn.BatchNorm1d(out_features, momentum=bn_momentum)\n",
        "        else:\n",
        "            self.skip = nn.Identity()\n",
        "            self.skip_bn = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        identity = self.skip(x)\n",
        "        if self.skip_bn is not None:\n",
        "            identity = self.skip_bn(identity)\n",
        "        \n",
        "        out = self.fc(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out + identity\n",
        "        return out\n",
        "\n",
        "\n",
        "class PatentNoveltyNet(nn.Module):\n",
        "    \"\"\"Deep neural network for patent novelty classification.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: List[int] = [128, 64, 32],\n",
        "        dropout: float = 0.3,\n",
        "        use_residual: bool = True,\n",
        "        bn_momentum: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.input_bn = nn.BatchNorm1d(input_dim, momentum=bn_momentum)\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_dims:\n",
        "            if use_residual:\n",
        "                layers.append(ResidualBlock(prev_dim, hidden_dim, dropout, bn_momentum))\n",
        "            else:\n",
        "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "                layers.append(nn.BatchNorm1d(hidden_dim, momentum=bn_momentum))\n",
        "                layers.append(nn.ReLU())\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        self.hidden_layers = nn.Sequential(*layers)\n",
        "        self.output_bn = nn.BatchNorm1d(prev_dim, momentum=bn_momentum)\n",
        "        self.output_layer = nn.Linear(prev_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        \"\"\"Xavier initialization for better convergence.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.input_bn(x)\n",
        "        x = self.hidden_layers(x)\n",
        "        x = self.output_bn(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PyTorchPatentClassifier:\n",
        "    \"\"\"PyTorch-based patent novelty classifier with modern deep learning techniques.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dims: List[int] = [128, 64, 32],\n",
        "        dropout: float = 0.3,\n",
        "        learning_rate: float = 0.001,\n",
        "        weight_decay: float = 1e-4,\n",
        "        batch_size: int = 256,\n",
        "        max_epochs: int = 100,\n",
        "        patience: int = 15,\n",
        "        use_residual: bool = True,\n",
        "        bn_momentum: float = 0.1,\n",
        "        device: str = None\n",
        "    ):\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epochs = max_epochs\n",
        "        self.patience = patience\n",
        "        self.use_residual = use_residual\n",
        "        self.bn_momentum = bn_momentum\n",
        "        \n",
        "        if device is None:\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "            elif torch.backends.mps.is_available():\n",
        "                self.device = torch.device(\"mps\")\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "        \n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.training_history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "        self.feature_names = None\n",
        "        \n",
        "        logger.info(f\"PyTorch Classifier initialized (device: {self.device})\")\n",
        "    \n",
        "    def _create_dataloaders(\n",
        "        self,\n",
        "        X_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        X_val: np.ndarray = None,\n",
        "        y_val: np.ndarray = None\n",
        "    ) -> Tuple[DataLoader, Optional[DataLoader]]:\n",
        "        X_train_t = torch.FloatTensor(X_train)\n",
        "        y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "        \n",
        "        train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            shuffle=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        \n",
        "        val_loader = None\n",
        "        if X_val is not None and y_val is not None:\n",
        "            X_val_t = torch.FloatTensor(X_val)\n",
        "            y_val_t = torch.FloatTensor(y_val).unsqueeze(1)\n",
        "            val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        \n",
        "        return train_loader, val_loader\n",
        "    \n",
        "    def fit(\n",
        "        self,\n",
        "        X_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        X_val: np.ndarray = None,\n",
        "        y_val: np.ndarray = None,\n",
        "        feature_names: List[str] = None,\n",
        "        use_mixup: bool = True,\n",
        "        mixup_alpha: float = 0.2\n",
        "    ) -> Dict:\n",
        "        self.feature_names = feature_names\n",
        "        \n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_val_scaled = self.scaler.transform(X_val) if X_val is not None else None\n",
        "        \n",
        "        input_dim = X_train.shape[1]\n",
        "        self.model = PatentNoveltyNet(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dims=self.hidden_dims,\n",
        "            dropout=self.dropout,\n",
        "            use_residual=self.use_residual,\n",
        "            bn_momentum=self.bn_momentum\n",
        "        ).to(self.device)\n",
        "        \n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        \n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        \n",
        "        train_loader, val_loader = self._create_dataloaders(\n",
        "            X_train_scaled, y_train, X_val_scaled, y_val\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Training PyTorch model...\")\n",
        "        logger.info(f\"  Architecture: {self.hidden_dims}\")\n",
        "        logger.info(f\"  Dropout: {self.dropout}\")\n",
        "        logger.info(f\"  Residual connections: {self.use_residual}\")\n",
        "        logger.info(f\"  Mixup augmentation: {use_mixup}\")\n",
        "        \n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        best_state = None\n",
        "        \n",
        "        for epoch in range(self.max_epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            num_batches = 0\n",
        "            \n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                \n",
        "                if use_mixup and np.random.random() > 0.5:\n",
        "                    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "                    index = torch.randperm(batch_X.size(0)).to(self.device)\n",
        "                    batch_X = lam * batch_X + (1 - lam) * batch_X[index]\n",
        "                    batch_y = lam * batch_y + (1 - lam) * batch_y[index]\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                \n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                num_batches += 1\n",
        "            \n",
        "            avg_train_loss = train_loss / num_batches if num_batches > 0 else 0.0\n",
        "            self.training_history[\"train_loss\"].append(avg_train_loss)\n",
        "            \n",
        "            if val_loader is not None:\n",
        "                self.model.eval()\n",
        "                val_loss = 0.0\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for batch_X, batch_y in val_loader:\n",
        "                        batch_X = batch_X.to(self.device)\n",
        "                        batch_y = batch_y.to(self.device)\n",
        "                        \n",
        "                        outputs = self.model(batch_X)\n",
        "                        loss = criterion(outputs, batch_y)\n",
        "                        val_loss += loss.item()\n",
        "                        \n",
        "                        all_preds.extend(outputs.cpu().numpy())\n",
        "                        all_labels.extend(batch_y.cpu().numpy())\n",
        "                \n",
        "                avg_val_loss = val_loss / len(val_loader)\n",
        "                val_acc = accuracy_score(\n",
        "                    np.array(all_labels) > 0.5,\n",
        "                    np.array(all_preds) > 0.5\n",
        "                )\n",
        "                \n",
        "                self.training_history[\"val_loss\"].append(avg_val_loss)\n",
        "                self.training_history[\"val_acc\"].append(val_acc)\n",
        "                \n",
        "                scheduler.step(avg_val_loss)\n",
        "                \n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    patience_counter = 0\n",
        "                    best_state = self.model.state_dict().copy()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                \n",
        "                if (epoch + 1) % 5 == 0:\n",
        "                    logger.info(\n",
        "                        f\"Epoch {epoch+1}/{self.max_epochs} - \"\n",
        "                        f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "                        f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "                        f\"Val Acc: {val_acc:.4f}\"\n",
        "                    )\n",
        "                \n",
        "                if patience_counter >= self.patience:\n",
        "                    logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "        \n",
        "        if best_state is not None:\n",
        "            self.model.load_state_dict(best_state)\n",
        "        \n",
        "        logger.info(\"Training complete!\")\n",
        "        \n",
        "        return self.training_history\n",
        "    \n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        self.model.eval()\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            probs = self.model(X_tensor).cpu().numpy()\n",
        "        \n",
        "        return np.hstack([1 - probs, probs])\n",
        "    \n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        probs = self.predict_proba(X)[:, 1]\n",
        "        return (probs > 0.5).astype(int)\n",
        "    \n",
        "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict:\n",
        "        y_pred = self.predict(X)\n",
        "        y_proba = self.predict_proba(X)[:, 1]\n",
        "        \n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(y, y_pred),\n",
        "            \"precision\": precision_score(y, y_pred),\n",
        "            \"recall\": recall_score(y, y_pred),\n",
        "            \"f1\": f1_score(y, y_pred),\n",
        "            \"roc_auc\": roc_auc_score(y, y_proba),\n",
        "            \"confusion_matrix\": confusion_matrix(y, y_pred).tolist()\n",
        "        }\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        path = Path(path)\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        torch.save({\n",
        "            \"model_state_dict\": self.model.state_dict(),\n",
        "            \"hidden_dims\": self.hidden_dims,\n",
        "            \"dropout\": self.dropout,\n",
        "            \"use_residual\": self.use_residual,\n",
        "            \"bn_momentum\": self.bn_momentum,\n",
        "            \"input_dim\": self.model.input_bn.num_features\n",
        "        }, path / \"pytorch_model.pt\")\n",
        "        \n",
        "        with open(path / \"scaler_pytorch.pkl\", \"wb\") as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        \n",
        "        with open(path / \"training_history_pytorch.json\", \"w\") as f:\n",
        "            json.dump(self.training_history, f, indent=2)\n",
        "        \n",
        "        logger.info(f\"Model saved to {path}\")\n",
        "    \n",
        "    def load(self, path: str):\n",
        "        path = Path(path)\n",
        "        \n",
        "        checkpoint = torch.load(path / \"pytorch_model.pt\", map_location=self.device)\n",
        "        \n",
        "        self.hidden_dims = checkpoint[\"hidden_dims\"]\n",
        "        self.dropout = checkpoint[\"dropout\"]\n",
        "        self.use_residual = checkpoint[\"use_residual\"]\n",
        "        self.bn_momentum = checkpoint.get(\"bn_momentum\", 0.1)\n",
        "        \n",
        "        self.model = PatentNoveltyNet(\n",
        "            input_dim=checkpoint[\"input_dim\"],\n",
        "            hidden_dims=self.hidden_dims,\n",
        "            dropout=self.dropout,\n",
        "            use_residual=self.use_residual,\n",
        "            bn_momentum=self.bn_momentum\n",
        "        ).to(self.device)\n",
        "        \n",
        "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        \n",
        "        with open(path / \"scaler_pytorch.pkl\", \"rb\") as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "        \n",
        "        logger.info(f\"Model loaded from {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_dir = Path(\"data/features\")\n",
        "\n",
        "X_train = np.load(features_dir / \"train_features_v2.X.npy\")\n",
        "y_train = np.load(features_dir / \"train_features_v2.y.npy\")\n",
        "X_val = np.load(features_dir / \"val_features_v2.X.npy\")\n",
        "y_val = np.load(features_dir / \"val_features_v2.y.npy\")\n",
        "X_test = np.load(features_dir / \"test_features_v2.X.npy\")\n",
        "y_test = np.load(features_dir / \"test_features_v2.y.npy\")\n",
        "\n",
        "old_feature_names = [\n",
        "    'bm25_doc_score',\n",
        "    'bm25_best_claim_score',\n",
        "    'cosine_doc_similarity',\n",
        "    'cosine_max_claim_similarity',\n",
        "    'embedding_diff_mean',\n",
        "    'embedding_diff_std',\n",
        "    'cpc_jaccard',\n",
        "    'year_diff',\n",
        "    'title_jaccard',\n",
        "    'abstract_length_ratio',\n",
        "    'claim_count_ratio',\n",
        "    'shared_rare_terms_ratio',\n",
        "    'claim_similarity'\n",
        "]\n",
        "\n",
        "print(f\"   Original features: {len(old_feature_names)}\")\n",
        "print(f\"   Original feature names: {old_feature_names}\")\n",
        "\n",
        "assert X_train.shape[1] == 13, f\"Expected 13 features, got {X_train.shape[1]}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove BM25 and CPC Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indices_to_remove = [0, 1, 6]\n",
        "indices_to_keep = [i for i in range(13) if i not in indices_to_remove]\n",
        "\n",
        "print(f\"\\n2. Removing features at indices {indices_to_remove}\")\n",
        "print(f\"   Removing: {[old_feature_names[i] for i in indices_to_remove]}\")\n",
        "\n",
        "X_train = X_train[:, indices_to_keep]\n",
        "X_val = X_val[:, indices_to_keep]\n",
        "X_test = X_test[:, indices_to_keep]\n",
        "\n",
        "feature_names = [old_feature_names[i] for i in indices_to_keep]\n",
        "\n",
        "print(f\"   Kept features: {feature_names}\")\n",
        "print(f\"   New feature count: {len(feature_names)}\")\n",
        "\n",
        "with open(features_dir / \"feature_names_v2.json\", \"w\") as f:\n",
        "    json.dump(feature_names, f, indent=2)\n",
        "print(f\"   Updated feature_names_v2.json with 10 features\")\n",
        "\n",
        "print(f\"\\n   Training set: {len(X_train)} samples, {X_train.shape[1]} features\")\n",
        "print(f\"   Validation set: {len(X_val)} samples\")\n",
        "print(f\"   Test set: {len(X_test)} samples\")\n",
        "\n",
        "assert X_train.shape[1] == 10, f\"Expected 10 features, got {X_train.shape[1]}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = PyTorchPatentClassifier(\n",
        "    hidden_dims=[128, 64, 32],\n",
        "    dropout=0.3,\n",
        "    learning_rate=0.001,\n",
        "    max_epochs=100,\n",
        "    patience=15,\n",
        "    batch_size=256\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n4. Training model...\")\n",
        "print(\"   This will take 10-20 minutes on GPU, 30-60 minutes on CPU\")\n",
        "start_time = time.time()\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    X_val, y_val,\n",
        "    feature_names=feature_names,\n",
        "    use_mixup=True\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\n   Training completed in {training_time/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n5. Evaluating model...\")\n",
        "train_metrics = model.evaluate(X_train, y_train)\n",
        "val_metrics = model.evaluate(X_val, y_val)\n",
        "test_metrics = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTraining Set:\")\n",
        "print(f\"  Accuracy:  {train_metrics['accuracy']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {train_metrics['roc_auc']:.4f}\")\n",
        "print(f\"  F1:        {train_metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nValidation Set:\")\n",
        "print(f\"  Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {val_metrics['roc_auc']:.4f}\")\n",
        "print(f\"  F1:        {val_metrics['f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Set:\")\n",
        "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
        "print(f\"  F1:        {test_metrics['f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n6. Saving model...\")\n",
        "model_path = Path(\"models/pytorch_nn\")\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "model.save(model_path)\n",
        "\n",
        "results = {\n",
        "    \"training_time_minutes\": training_time / 60,\n",
        "    \"features\": feature_names,\n",
        "    \"num_features\": 10,\n",
        "    \"removed_features\": [old_feature_names[i] for i in indices_to_remove],\n",
        "    \"train_metrics\": train_metrics,\n",
        "    \"val_metrics\": val_metrics,\n",
        "    \"test_metrics\": test_metrics,\n",
        "    \"timestamp\": datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "results_path = model_path / \"training_results.json\"\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"   Model saved to: {model_path}\")\n",
        "print(f\"   Results saved to: {results_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nModel retrained with 10 features (BM25 and CPC removed)\")\n",
        "print(f\"Feature names updated in: data/features/feature_names_v2.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
